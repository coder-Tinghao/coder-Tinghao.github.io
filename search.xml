<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>GFS论文总结</title>
    <url>/2023/02/13/GFS%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<h1 id="GFS论文总结"><a href="#GFS论文总结" class="headerlink" title="GFS论文总结"></a>GFS论文总结</h1><p>GFS（The Google File System），Google的目标是构建一个大型的，快速的文件系统。并且这个文件系统是全局有效的，这样各种不同的应用程序都可以从中读取数据。从硬件到使用了GFS的软件都有讨论，并且它也是一个成功的现实世界的设计。尽管这是在学术会议上发表的学术论文，但是文章里介绍的东西（GFS）也相当成功，并且在现实世界中使用了相当长的时间。</p>
<h1 id="分布式存储系统"><a href="#分布式存储系统" class="headerlink" title="分布式存储系统"></a>分布式存储系统</h1><p>存储是一种关键的抽象，可能有各种各样重要的抽象可以应用在分布式系统中，简单的存储接口往往非常有用且极其通用。所以，构建分布式系统大多都是关于如何设计存储系统，或是设计其它基于大型分布式存储的系统。所以我们会更加关注如何为大型分布式存储系统设计一个优秀的接口，以及如何设计存储系统的内部结构，这样系统才能良好运行。<br><img data-src="https://github.com/coder-Tinghao/coder-Tinghao.github.io/blob/main/images/distributed_system.png?raw=true" alt="分布式系统"></p>
<p>1.对于分布式存储，为了获取更大的性能，利用数百台计算机的资源来同时完成大量工作</p>
<p>2.将数据分割放到大量的服务器上，这样就可以并行的从多台服务器读取数据。我们将这种方式称之为分片（Sharding）</p>
<p>3.分片导致常态的错误，需要自动化的方法而不是人工介入来修复错误，引出容错</p>
<p>4.实现容错最有用的一种方法是使用复制，只需要维护2-3个数据的副本，当其中一个故障了，你就可以使用另一个。所以，如果想要容错能力，就得有复制（replication）</p>
<p>5.如果有复制，那就有了两份数据的副本。可以确定的是，如果你不小心，它们就会不一致。获取到的数据内容也将取决于你向哪个副本请求数据。这对于应用程序来说就有些麻烦了。所以，如果我们有了复制，我们就有不一致的问题（inconsistency）</p>
<p>6.避免不一致的问题，并且让数据看起来也表现的符合预期。但是为了达到这样的效果，你总是需要额外的工作，需要不同服务器之间通过网络额外的交互，而这样的交互会降低性能。所以如果你想要一致性，你的代价就是低性能。</p>
<p><strong>通常，人们很少会乐意为好的一致性付出相应的性能代价。</strong></p>
<h1 id="GFS系统设计"><a href="#GFS系统设计" class="headerlink" title="GFS系统设计"></a>GFS系统设计</h1><p><img data-src="https://github.com/coder-Tinghao/coder-Tinghao.github.io/blob/main/images/GFS_architecture.png?raw=true" alt="GFS结构"></p>
<h2 id="GFS设计目标"><a href="#GFS设计目标" class="headerlink" title="GFS设计目标"></a>GFS设计目标</h2><p>为了获得大容量和高速的特性，每个包含了数据的文件会被GFS自动的分割并存放在多个服务器之上，这样读写操作自然就会变得很快。因为可以从多个服务器上同时读取同一个文件，进而获得更高的聚合吞吐量。将文件分割存储还可以在存储系统中保存比单个磁盘还要大的文件。</p>
<p><strong>GFS是为TB级别的文件而生。并且GFS只会顺序处理，不支持随机访问</strong></p>
<h2 id="GFS-Master节点"><a href="#GFS-Master节点" class="headerlink" title="GFS Master节点"></a>GFS Master节点</h2><p>Master节点用来管理文件和Chunk的信息，而Chunk服务器用来存储实际的数据。Master节点知道每一个文件对应的所有的Chunk的ID，这些Chunk每个是64MB大小，它们共同构成了一个文件。如果我有一个1GB的文件，那么Master节点就知道文件的第一个Chunk存储在哪，第二个Chunk存储在哪，等等。当我想读取这个文件中的任意一个部分时，我需要向Master节点查询对应的Chunk在哪个服务器上，之后我可以直接从Chunk服务器读取对应的Chunk数据。</p>
<p>Master节点主要保存了两个表单：</p>
<p>1.文件名到Chunk ID或者Chunk Handle数组的对应。这个表单告诉你，文件对应了哪些Chunk。</p>
<p>2.记录了Chunk ID到Chunk数据的对应关系。这里的数据又包括了：<br><strong>每个Chunk存储在哪些服务器上</strong>；<strong>每个Chunk当前的版本号</strong>；<strong>哪个Chunk服务器持有主Chunk</strong>，所有对于Chunk的写操作都必须在主Chunk（Primary Chunk）上顺序处理，主Chunk是Chunk的多个副本之一；<br><strong>主Chunk的租约过期时间</strong>，主Chunk只能在特定的租约时间内担任主Chunk</p>
<p>Master会在磁盘上存储log，每次有数据变更时，Master会在磁盘的log中追加一条记录，并生成CheckPoint（类似于备份点）</p>
<p><img data-src="https://github.com/coder-Tinghao/coder-Tinghao.github.io/blob/main/images/GFS_Master.png?raw=true" alt="GFS_Master"></p>
<p>有些数据需要存在磁盘上，而有些不用。它们分别是：<br>如图所示，需要保存在磁盘上的，标记为NV（non-volatile, 非易失）；不需要保存在磁盘上的标记为V</p>
<p>任何时候，如果文件扩展到达了一个新的64MB，需要新增一个Chunk或者由于指定了新的主Chunk而导致版本号更新了，Master节点需要向磁盘中的Log追加一条记录。这里在磁盘中维护log而不是数据库的原因是，数据库本质上来说是某种B树（b-tree）或者hash table，相比之下，<strong>追加log会非常的高效</strong>，因为你可以将最近的<strong>多个log记录一次性的写入磁盘</strong>。因为这些数据都是向同一个地址追加，这样只需要等待磁盘的磁碟旋转一次。而对于B树来说，每一份数据都需要在磁盘中随机找个位置写入。</p>
<p>Master节点重启时，会从log中的最近一个<strong>checkpoint</strong>开始恢复，再逐条执行从Checkpoint开始的log，最后恢复自己的状态。</p>
<h2 id="GFS读文件"><a href="#GFS读文件" class="headerlink" title="GFS读文件"></a>GFS读文件</h2><p>读请求，意味着应用程序或者GFS客户端有一个文件名和它想从文件的某个位置读取的偏移量（offset），应用程序会将这些信息发送给Master节点。Master节点会从自己的file表单中查询文件名，得到Chunk ID的数组。因为每个Chunk是64MB，所以偏移量除以64MB就可以从数组中得到对应的Chunk ID。之后Master再从Chunk表单中找到存有Chunk的服务器列表，并将列表返回给客户端。所以，第一步是客户端（或者应用程序）将文件名和偏移量发送给Master。第二步，Master节点将Chunk Handle（也就是ID，记为H）和服务器列表发送给客户端。</p>
<p>客户端可以从这些Chunk服务器中挑选一个来读取数据。GFS论文说，客户端会选择一个网络上最近的服务器（Google的数据中心中，IP地址是连续的，所以可以从IP地址的差异判断网络位置的远近），并将读请求发送到那个服务器。因为客户端每次可能只读取1MB或者64KB数据，所以，客户端可能会连续多次读取同一个Chunk的不同位置。所以，客户端会缓存Chunk和服务器的对应关系。</p>
<p>客户端会与选出的Chunk服务器通信，将Chunk Handle和偏移量发送给那个Chunk服务器。Chunk服务器会在本地的硬盘上，将每个Chunk存储成独立的Linux文件，并通过普通的Linux文件系统管理</p>
<h2 id="GFS写文件"><a href="#GFS写文件" class="headerlink" title="GFS写文件"></a>GFS写文件</h2><p>写文件，客户端会向Master节点发送请求说：我想向这个文件名对应的文件追加数据，请告诉我文件中<strong>最后一个Chunk的位置</strong>。这就是GFS论文中讨论的<strong>记录追加（Record Append）</strong></p>
<p><img data-src="https://github.com/coder-Tinghao/coder-Tinghao.github.io/blob/main/images/GFS_write.png?raw=true" alt="GFS写操作"></p>
<p>GFS 使用了**租约(lease)**的手段，Master 会定期向 chunk 的某个 replica 所在的服务器进行授权（有超时时间，所以称为租约），拿到授权的副本称为主副本（primary replica），由其进行写入顺序的安排。</p>
<p>对于读文件来说，可以从任何最新的Chunk副本读取数据，但是对于写文件来说，必须要通过Chunk的主副本（Primary Chunk）来写入。对于Master节点来说，如果发现Chunk的主副本不存在，Master会找出所有存有Chunk最新副本的Chunk服务器。每个Chunk可能同时有多个副本，最新的副本是指，副本中保存的版本号与Master中记录的Chunk的版本号一致。当客户端想要对文件进行追加，但是又不知道文件尾的Chunk对应的Primary在哪时，Master会等所有存储了最新Chunk版本的服务器集合完成，然后挑选一个作为Primary，其他的作为Secondary。</p>
<p>现在我们有了一个Primary，它可以接收来自客户端的写请求，并将写请求应用在多个Chunk服务器中。这样Master可以将实际更新Chunk的能力转移给Primary服务器。并且在将版本号更新到Primary和Secondary服务器之后，如果Master节点故障重启，还是可以在相同的Primary和Secondary服务器上继续更新Chunk。</p>
<h2 id="GFS一致性"><a href="#GFS一致性" class="headerlink" title="GFS一致性"></a>GFS一致性</h2><p><img data-src="https://github.com/coder-Tinghao/coder-Tinghao.github.io/blob/main/images/GFS_consistency.png?raw=true" alt="GFS一致性"></p>
<p>如图所示，追加A成功，追加B时第三个副本失败，追加C成功，而后重新追加B会在C的后面追加。</p>
<p>在GFS的这种工作方式下，如果Primary返回写入成功，那么一切都还好，如果Primary返回写入失败，就不是那么好了。Primary返回写入失败会导致不同的副本有完全不同的数据。</p>
<h2 id="GFS问题"><a href="#GFS问题" class="headerlink" title="GFS问题"></a>GFS问题</h2><p>它最严重的局限可能在于，它只有一个Master节点，会带来以下问题：</p>
<p>·Master节点必须为每个文件，每个Chunk维护表单，随着GFS的应用越来越多，涉及的文件也越来越多，最终Master会耗尽内存来存储文件表单。你可以增加内存，但是单台计算机的内存也是有上限的。</p>
<p>·单个Master节点要承载数千个客户端的请求，而Master节点的CPU每秒只能处理数百个请求，尤其Master还需要将部分数据写入磁盘，很快，客户端数量超过了单个Master的能力。</p>
<p>·应用程序发现很难处理GFS奇怪的语义（本节最开始介绍的GFS的副本数据的同步，或者可以说不同步）。</p>
<p>·Master节点的故障切换不是自动的。GFS需要人工干预来处理已经永久故障的Master节点，并更换新的服务器，这可能需要几十分钟甚至更长的而时间来处理。对于某些应用程序来说，这个时间太长了。</p>
]]></content>
      <tags>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title>Mapreduce论文总结</title>
    <url>/2023/02/05/Mapreduce%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>最近决定通过MIT6.824的分布式课程学习一下Google的三篇经典论文，Mapreduce、GFS、Raft。6.824课程相关链接如下：</p>
<p><a href="https://mit-public-courses-cn-translatio.gitbook.io/mit6-824/lecture-01-introduction">6.824课程翻译链接</a></p>
<p><a href="https://www.bilibili.com/video/av87684880/?vd_source=cc4910ad55a90281a3f278b41a379e5b">2020课程视频链接（带字幕）</a></p>
<p><a href="http://nil.csail.mit.edu/6.824/2020/schedule.html">2020课程安排官方链接（含实验）</a></p>
<p>起初读到Mapreduce论文的时候还在12月初，想要去总结但一拖拖到现在，接下来我将对Mapreduce论文的学习进行一些总结：</p>
<h1 id="一致性"><a href="#一致性" class="headerlink" title="一致性"></a>一致性</h1><p>一致性就是用来定义操作行为的概念。从性能和容错的角度来说，我们通常会有多个副本。在一个非分布式系统中，你通常只有一个服务器，一个表单。虽然不是绝对，但是通常来说对于put&#x2F;get的行为不会有歧义。直观上来说，put就是更新这个表单，get就是从表单中获取当前表单中存储的数据。但是在一个分布式系统中，由于复制或者缓存，数据可能存在于多个副本当中，于是就有了多个不同版本的key-value对。假设服务器有两个副本，那么他们都有一个key-value表单，两个表单中key 1对应的值都是20。</p>
<p>之后会发送给第二台服务器，因为相同的put请求需要发送给两个副本，这样这两个副本才能保持同步。但是就在客户端准备给第二台服务器发送相同请求时，这个客户端故障了，可能是电源故障或者操作系统的bug之类的。所以，现在我们处于一个不好的状态，我们发送了一个put请求，更新了一个副本的值是21，但是另一个副本的值仍然是20。</p>
<p>如果现在某人通过get读取key为1的值，那么他可能获得21，也可能获得20，取决于get请求发送到了哪个服务器。即使规定了总是把请求先发送给第一个服务器，那么我们在构建容错系统时，如果第一台服务器故障了，请求也会发给第二台服务器。所以不管怎么样，总有一天你会面临暴露旧数据的风险。很可能是这样，最开始许多get请求都得到了21，之后过了一周突然一些get请求得到了一周之前的旧数据（20）。所以，这里不是很一致。并且，如果我们不小心的话，这个场景是可能发生的，所以，我们需要确定put&#x2F;get操作的一些规则。</p>
<p>实际上，对于一致性有很多不同的定义。有一些非常直观，比如说get请求可以得到最近一次完成的put请求写入的值。这种一般也被称为强一致（Strong Consistency）。弱一致是指，不保证get请求可以得到最近一次完成的put请求写入的值。尽管有很多细节的工作要处理，强一致可以保证get得到的是put写入的最新的数据；而很多的弱一致系统不会做出类似的保证。所以在一个弱一致系统中，某人通过put请求写入了一个数据，但是你通过get看到的可能仍然是一个旧数据，而这个旧数据可能是很久之前写入的。</p>
<p><img data-src="https://github.com/coder-Tinghao/coder-Tinghao.github.io/blob/main/images/consistency.png?raw=true" alt="一致性"></p>
<h1 id="Mapreduce论文"><a href="#Mapreduce论文" class="headerlink" title="Mapreduce论文"></a>Mapreduce论文</h1><h2 id="Mapreduce框架"><a href="#Mapreduce框架" class="headerlink" title="Mapreduce框架"></a>Mapreduce框架</h2><p>MapReduce的思想是：应用程序设计人员和分布式运算的使用者，只需要写简单的Map函数和Reduce函数，而不需要知道任何有关分布式的事情，MapReduce框架会处理剩下的事情。</p>
<p><img data-src="https://github.com/coder-Tinghao/coder-Tinghao.github.io/blob/main/images/mapreduce.png?raw=true" alt="Mapreduce框架"></p>
<p>Map函数以文件作为输入，文件是整个输入数据的一部分。Map函数的输出是一个key-value对的列表。假设我们在实现一个最简单的MapReduce Job：单词计数器。它会统计每个单词出现的次数。在这个例子中，Map函数会输出key-value对，其中key是单词，而value是1。Map函数会将输入中的每个单词拆分，并输出一个key-value对，key是该单词，value是1。最后需要对所有的key-value进行计数，以获得最终的输出。所以，假设输入文件1包含了单词a和单词b，Map函数的输出将会是key&#x3D;a，value&#x3D;1和key&#x3D;b，value&#x3D;1。第二个Map函数只从输入文件2看到了b，那么输出将会是key&#x3D;b，value&#x3D;1。第三个输入文件有一个a和一个c。</p>
<p><img data-src="https://github.com/coder-Tinghao/coder-Tinghao.github.io/blob/main/images/mapreduce_execution.png?raw=true" alt="Mapreduce执行过程"></p>
<p>我们对所有的输入文件都运行了Map函数，并得到了论文中称之为中间输出（intermediate output），也就是每个Map函数输出的key-value对。<br>运算的第二阶段是运行Reduce函数。MapReduce框架会收集所有Map函数输出的每一个单词的统计。比如说，MapReduce框架会先收集每一个Map函数输出的key为a的key-value对。收集了之后，会将它们提交给Reduce函数。</p>
<p>之后会收集所有的b。这里的收集是真正意义上的收集，因为b是由不同计算机上的不同Map函数生成，所以不仅仅是数据从一台计算机移动到另一台（如果Map只在一台计算机的一个实例里，可以直接通过一个RPC将数据从Map移到Reduce）。我们收集所有的b，并将它们提交给另一个Reduce函数。这个Reduce函数的入参是所有的key为b的key-value对。对c也是一样。所以，MapReduce框架会为所有Map函数输出的每一个key，调用一次Reduce函数。</p>
<p>Job：整个MapReduce计算称为Job</p>
<p>Task：每一次MapReduce调用称为Task</p>
<h2 id="Map函数和Reduce函数"><a href="#Map函数和Reduce函数" class="headerlink" title="Map函数和Reduce函数"></a>Map函数和Reduce函数</h2><h3 id="Map函数"><a href="#Map函数" class="headerlink" title="Map函数"></a>Map函数</h3><p>入参中，key是输入文件的名字，通常会被忽略，因为我们不太关心文件名是什么，value是输入文件的内容。所以，对于一个单词计数器来说，value包含了要统计的文本，我们会将这个文本拆分成单词。之后对于每一个单词，我们都会调用emit。emit由MapReduce框架提供，并且这里的emit属于Map函数。emit会接收两个参数，其中一个是key，另一个是value。在单词计数器的例子中，emit入参的key是单词，value是字符串“1”。</p>
<p><img data-src="https://github.com/coder-Tinghao/coder-Tinghao.github.io/blob/main/images/map.png?raw=true" alt="Map函数"></p>
<h3 id="Reduce函数"><a href="#Reduce函数" class="headerlink" title="Reduce函数"></a>Reduce函数</h3><p>Reduce函数的入参是某个特定key的所有实例（Map输出中的key-value对中，出现了一次特定的key就可以算作一个实例）。所以Reduce函数也是使用一个key和一个value作为参数，其中value是一个数组，里面每一个元素是Map函数输出的key的一个实例的value。对于单词计数器来说，key就是单词，value就是由字符串“1”组成的数组，所以，我们不需要关心value的内容是什么，我们只需要关心value数组的长度。Reduce的emit函数只接受一个参数value，这个value会作为Reduce函数入参的key的最终输出。所以，对于单词计数器，我们会给emit传入数组的长度。</p>
<p><img data-src="https://github.com/coder-Tinghao/coder-Tinghao.github.io/blob/main/images/reduce.png?raw=true" alt="Reduce函数"></p>
<h2 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h2><h3 id="Shuffle过程"><a href="#Shuffle过程" class="headerlink" title="Shuffle过程"></a>Shuffle过程</h3><p>Mapreduce框架中提到的从Map函数到Reduce函数的过程就是<strong>shuffle</strong>，根据课程中的讲解，这个过程如mapreduce图所示<strong>本质是从行存储到列存储的转换</strong>，也是mapreduce整个过程代价最大的部分</p>
<p>这里的shuffle的重点是，这里实际上可能会有大量的网络通信。假设你在进行排序，排序的输入输出会有相同的大小。这样，如果你的输入是10TB，为了能排序，你需要将10TB的数据在网络上移动，并且输出也会是10TB，所以这里有大量的数据。这可能发生在任何MapReduce job中，尽管有一些MapReduce job在不同阶段的数据没有那么大。之前有人提过，想将Reduce的输出传给另一个MapReduce job，而这也是人们常做的事情。在一些场景中，Reduce的输出可能会非常巨大，比如排序，比如网页索引器。10TB的输入对应的是10TB的输出。所以，Reduce的输出也会存储在GFS上。但是Reduce只会生成key-value对，MapReduce框架会收集这些数据，并将它们写入到GFS的大文件中。</p>
<h3 id="Mapreduce-GFS"><a href="#Mapreduce-GFS" class="headerlink" title="Mapreduce+GFS"></a>Mapreduce+GFS</h3><p>GFS是一个共享文件服务，并且它也运行在MapReduce的worker集群的物理服务器上。GFS会自动拆分你存储的任何大文件，并且以64MB的块存储在多个服务器之上。所以，如果你有了10TB的网页数据，你只需要将它们写入到GFS，甚至你写入的时候是作为一个大文件写入的，GFS会自动将这个大文件拆分成64MB的块，并将这些块平均的分布在所有的GFS服务器之上，而这是极好的，这正是我们所需要的。如果我们接下来想要对刚刚那10TB的网页数据运行MapReduce Job，数据已经均匀的分割存储在所有的服务器上了。如果我们有1000台服务器，我们会启动1000个Map worker，每个Map worker会读取1&#x2F;1000输入数据。这些Map worker可以并行的从1000个GFS文件服务器读取数据，并获取巨大的读取吞吐量，也就是1000台服务器能提供的吞吐量。</p>
<h3 id="论文补充"><a href="#论文补充" class="headerlink" title="论文补充"></a>论文补充</h3><p>The master keeps several data structures. For each map task and reduce task, it stores the state (idle, in-progress, or completed), and the identity of the worker machine (for non-idle tasks).</p>
<p>The master pings every worker periodically. If no re- sponse is received from a worker in a certain amount of time, the master marks the worker as failed.</p>
<p>The master runs an internal HTTP server and exports a set of status pages for human consumption.</p>
<h3 id="实验总结"><a href="#实验总结" class="headerlink" title="实验总结"></a>实验总结</h3><p><a href="http://nil.csail.mit.edu/6.824/2020/labs/lab-mr.html">6.824lab1链接</a></p>
]]></content>
      <tags>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2022/07/23/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
  </entry>
  <entry>
    <title>copy-on-write机制</title>
    <url>/2023/02/17/copy-on-write%E6%9C%BA%E5%88%B6/</url>
    <content><![CDATA[<h1 id="copy-on-write机制"><a href="#copy-on-write机制" class="headerlink" title="copy-on-write机制"></a>copy-on-write机制</h1><p><a href="https://juejin.cn/post/6844903702373859335">参考链接</a></p>
<p>写时复制（Copy-on-write，COW），有时也称为隐式共享（implicit sharing）。COW 将复制操作推迟到第一次写入时进行：在创建一个新副本时，不会立即复制资源，而是共享原始副本的资源；当修改时再执行复制操作。通过这种方式共享资源，可以显著减少创建副本时的开销，以及节省资源；同时，资源修改操作会增加少量开销。</p>
<h1 id="linux下的fork函数和exec函数"><a href="#linux下的fork函数和exec函数" class="headerlink" title="linux下的fork函数和exec函数"></a>linux下的fork函数和exec函数</h1><p>在说明Linux下的copy-on-write机制前，首先要知道两个函数：fork()和exec()</p>
<p>需要注意的是exec()并不是一个特定的函数, 它是一组函数的统称, 它包括了execl()、execlp()、execv()、execle()、execve()、execvp()</p>
<p>fork是类Unix操作系统上<strong>创建进程</strong>的主要方法，fork用于创建子进程(等同于当前进程的副本)，新的进程要通过老的进程复制自身得到，Linux的进程都通过<strong>init进程或init的子进程</strong>fork(vfork)出来的</p>
<p>从上面我们已经知道了fork会创建一个子进程。子进程的是父进程的副本。</p>
<p>exec函数的作用就是：装载一个新的程序（可执行映像）覆盖当前进程内存空间中的映像，从而执行不同的任务，exec系列函数在执行时会直接替换掉当前进程的地址空间。</p>
<h1 id="copy-on-write实现原理"><a href="#copy-on-write实现原理" class="headerlink" title="copy-on-write实现原理"></a>copy-on-write实现原理</h1><h2 id="设计原理"><a href="#设计原理" class="headerlink" title="设计原理"></a>设计原理</h2><p>既然很多时候复制给子进程的数据是无效的，于是就有了Copy On Write这项技术了，原理也很简单：</p>
<p>fork创建出的子进程，<strong>与父进程共享内存空间</strong>。也就是说，如果<strong>子进程不对内存空间进行写入操作的话，内存空间中的数据并不会复制给子进程</strong>，这样创建子进程的速度就很快了！(不用复制，直接引用父进程的物理空间)。<br>并且如果在fork函数返回之后，子进程<strong>第一时间</strong>exec一个新的可执行映像，那么也不会浪费时间和内存空间了。</p>
<p>另外表达：在fork之后exec之前两个进程用的是相同的<strong>物理空间（内存区）</strong>，子进程的代码段、数据段、堆栈都是指向父进程的物理空间，也就是说，两者的虚拟空间不同，但其对应的物理空间是同一个，当<strong>父子进程中有更改相应段的行为</strong>发生时，再为子进程相应的段分配物理空间</p>
<p><img data-src="https://github.com/coder-Tinghao/coder-Tinghao.github.io/blob/main/images/copy-on-write_implement.png?raw=true" alt="实现原理"></p>
<h2 id="实现原理"><a href="#实现原理" class="headerlink" title="实现原理"></a>实现原理</h2><p>fork()之后，kernel把父进程中所有的内存页的权限都设为<strong>read-only</strong>，然后子进程的地址空间指向父进程。当父子进程都只读内存时，相安无事。当其中某个进程写内存时，CPU硬件检测到内存页是read-only的，于是触发<strong>页异常中断（page-fault）</strong>，陷入kernel的一个中断例程。中断例程中，kernel就会把触发的异常的页复制一份，于是父子进程各自持有独立的一份。</p>
<h2 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h2><p>优点：减少不必要的资源分配，节省宝贵的物理内存。</p>
<p>缺点：如果在子进程存在期间发生了大量写操作，那么会频繁地产生页面错误，不断陷入内核，复制页面。这反而会降低效率。</p>
<h2 id="实际应用"><a href="#实际应用" class="headerlink" title="实际应用"></a>实际应用</h2><p><strong>语言中的应用:</strong><br>相比于传统的深层复制，能带来很大性能提升。比如 C++ 98 标准下的 std::string 就采用了写时复制的实现：</p>
<pre><code>std::string x(&quot;Hello&quot;);
std::string y = x;  // x、y 共享相同的 buffer
y += &quot;, World!&quot;;    // 写时复制，此时 y 使用一个新的 buffer
                    // x 依然使用旧的 buffer
</code></pre>
<p>Golang、PHP 中的 string、array 也是写时复制。在修改这些类型时，如果其引用计数非零，则会复制一个副本。因此我们在 golang、php 中可以将字符串、数组当作值类型（values type）进行传递，即不会有传值复制的开销，也能保证其 immutable 的特性。</p>
<p><strong>文件系统中的应用：</strong><br>Copy-on-write在对数据进行修改的时候，不会直接在原来的数据位置上进行操作，而是重新找个位置修改，这样的好处是一旦系统突然断电，重启之后不需要做Fsck。好处就是能保证数据的完整性，掉电的话容易恢复。</p>
<p>比如说：要修改数据块A的内容，先把A读出来，写到B块里面去。如果这时候断电了，原来A的内容还在！</p>
]]></content>
      <tags>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title>vm-ft论文总结</title>
    <url>/2023/02/27/vm-ft%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<h1 id="容错与复制"><a href="#容错与复制" class="headerlink" title="容错与复制"></a>容错与复制</h1><p>这节课首先谈论了容错（Fault-Tolerance）技术中的<strong>复制（Replication）技术</strong></p>
<p>目前我知道的分布式存储手段有<strong>复制（Replication）</strong>和<strong>分片（Sharding）</strong>，复制技术主要用于解决<strong>fail-stop故障</strong>，比如 CPU 过热而关闭、主机或者网络断电、硬盘空间耗尽等问题。但是复制不能处理一些<strong>相关联（correlated，主副本机器会同时存在）的问题和软件中的bug和硬件设计中的缺陷</strong>。</p>
<p>至于是否值得使用复制技术，需要对业务场景和所需费用考量，是否真的需要进行 Replica。比如银行数据就需要多备份，而课程网站可能并不需要。</p>
<h1 id="VM-FT论文总结"><a href="#VM-FT论文总结" class="headerlink" title="VM-FT论文总结"></a>VM-FT论文总结</h1><h2 id="状态转移和复制状态机"><a href="#状态转移和复制状态机" class="headerlink" title="状态转移和复制状态机"></a>状态转移和复制状态机</h2><p>论文首先介绍了两种进行状态备份的方式：</p>
<p><strong>状态转移（State transfer）</strong></p>
<p>Primary和Backup互为副本，Primary将自己完整状态，比如说内存中的内容，拷贝并发送给Backup。Backup会保存收到的<strong>最近一次状态</strong>。每过一会，Primary就会对自身的内存做一大份拷贝，并通过网络将其发送到Backup。为了提升效率，你可以想到每次同步只发送上次同步之后变更了的内存。</p>
<p><strong>复制状态机（Replicated State Machine）</strong></p>
<p>将服务器看作是一个具有确定性状态的状态机，只要给定相同初始状态和同样顺序的确定输入，就能保持同样的状态。同步的是<strong>外部的事件 &#x2F; 操作 &#x2F; 输入</strong>；同步的内容通常较小，但是依赖主机的一些特性：比如指令执行的确定性（deterministic）。而在物理机上保证确定性很难，但是在 VM 上就简单的多，由于 hypervisor 有对 VM 有完全的控制权，因此可以通过某些手段来额外同步某些不确定性输入（比如类似随机数、系统时钟等）。</p>
<p>Replicated State Machine 需要机器为<strong>单核</strong>，因为在多核机器上，指令的执行顺序本身是不确定的。那对于多核机器如何做同步？State Transfer </p>
<h2 id="同步状态的层级"><a href="#同步状态的层级" class="headerlink" title="同步状态的层级"></a>同步状态的层级</h2><p>应用层（Application state）。如 GFS，更为高效，只需要发送高维操作即可，缺点是需要在应用层进行容错。<br>机器层（Machine level）。可以让运行在服务器上的应用无需改动而获取容错能力。但需要细粒度的同步机器事件（中断、DMA）；并且需要修改机器底层实现以发送这些事件。<br>而 VM-FT 选择了后者，能力更强大，但也做出了更多牺牲。</p>
<p><img data-src="https://github.com/coder-Tinghao/coder-Tinghao.github.io/blob/main/images/vm-ft_configuration.png?raw=true" alt="vm-ft配置"></p>
<p>VM-FT 系统使用一个额外的虚拟层 <strong>VMMonitor</strong>（ hypervisor &#x2F; (VMM)Virtual Machine Monitor），当 client 请求到达 Primary 时，VMMonitor 一方面向本机转发请求、一方面向 Backup 的 VMMonitor 同步请求。处理完请求得到结果时，Primary 的 VMMonitor 会回复 Client，而 Backup 的 VMMonitor 会丢弃 Backup 产生的回复。</p>
<p>使用两种方法来检测 Primary 和 Backup 的健康状况：</p>
<p>· 和 Primary&#x2F;Backup 进行心跳<br>· 监控<strong>logging channel</strong></p>
<p>VMware FT论文中将<strong>Primary到Backup之间同步的数据流的通道称之为Log Channel</strong></p>
<h2 id="主从切换"><a href="#主从切换" class="headerlink" title="主从切换"></a>主从切换</h2><p>当Primary宕机时，Backup虚机会上线（Go Alive），Backup的VMM会让Backup自由执行，而不是受来自于Primary的事件驱动。Backup的VMM会在网络中做一些处理（猜测是发GARP），让后续的客户端请求发往Backup，而不是Primary。同时，Backup的VMM不再会丢弃Backup的输出。然后利用 VMotion 的技术在和新 Primary 共享外存的地方启动一个副本，并且建立日志通道。</p>
<p>一种可能是，Backup 声称具有 Primary 的 MAC 地址，然后让 ARP 缓存表过期，就将打向某个 IP 的流量从 Primary 切换到了 Backup。</p>
<p><img data-src="https://github.com/coder-Tinghao/coder-Tinghao.github.io/blob/main/images/wm-ft_workflow%20.png?raw=true" alt="vm-ft工作原理"></p>
<h2 id="非确定性事件"><a href="#非确定性事件" class="headerlink" title="非确定性事件"></a>非确定性事件</h2><p>所谓非确定性事件就是不由当前内存直接决定的指令。如果不够小心，这些指令在Primary和Backup的运行结果可能会不一样。</p>
<p>非确定性事件主要包括以下三种：</p>
<p>·<strong>客户端输入</strong></p>
<p>输入实际上是指接收到了一个<strong>网络数据包</strong>，网络数据包有两部分，一个是数据包中的数据，另一个是提示<strong>数据包送达的中断</strong>。当网络数据包送达时，通常网卡的DMA（Direct Memory Access）会将网络数据包的内容拷贝到内存，之后触发一个中断。</p>
<p><strong>·怪异指令</strong></p>
<p>随机数生成器，获取当前时间的指令，获取计算机的唯一ID</p>
<p><strong>·多CPU并发</strong></p>
<p>当服务运行在多CPU上时，指令在不同的CPU上会交织在一起运行，进而产生的指令顺序是不可预期的</p>
<p>所有的事件都需要通过<strong>Log Channel</strong>，从Primary同步到Backup，对于不确定性操作，需要保留充足的信息到日志通道中，以使 Backup 可以进行同样的状态改变，并且产生同样输出。发送到日志通道的事件信息包括：<strong>事件发生时的指令序号、日志条目的类型、数据</strong></p>
<p><strong>如何防止backup执行快于primary？</strong><br>VMware FT会维护一个来自于Primary的<strong>Log条目的等待缓冲区</strong>，如果缓冲区为空，Backup是不允许执行指令的。如果缓冲区不为空，那么它可以根据Log的信息知道Primary对应的指令序号，并且会强制Backup虚机最多执行指令到这个位置。</p>
<h2 id="输出控制（OutPut-rule）"><a href="#输出控制（OutPut-rule）" class="headerlink" title="输出控制（OutPut rule）"></a>输出控制（OutPut rule）</h2><p>在Primary收到客户端请求并生成数据后，VMM不会无条件转发这个输出给客户端。</p>
<p><img data-src="https://github.com/coder-Tinghao/coder-Tinghao.github.io/blob/main/images/FT_protocol.png?raw=true" alt="FT_protocol"></p>
<p><strong>Primary的VMM会等到之前的Log条目都被Backup虚机确认收到了才将输出转发给客户端</strong>。所以，包含了客户端输入的Log条目，会从Primary的VMM送到Backup的VMM，Backup的VMM不用等到Backup虚机实际执行这个输入，就会发送一个表明收到了这条Log的ACK报文给Primary的VMM。当Primary的VMM收到了这个ACK，才会将Primary虚机生成的输出转发到网络中。</p>
<p><strong>核心：确保在客户端看到对于请求的响应时，Backup虚机一定也看到了对应的请求</strong></p>
<h2 id="Test-and-Set服务"><a href="#Test-and-Set服务" class="headerlink" title="Test-and-Set服务"></a>Test-and-Set服务</h2><p>当Primary和Backup都在运行，但是它们之间的网络出现了问题，同时它们各自又能够与一些客户端通信。<strong>产生Split Brain现象</strong>，这篇论文的办法是：向一个<strong>外部的第三方权威机构求证，来决定Primary还是Backup允许上线</strong>。</p>
<p>Test-and-Set服务不运行在Primary和Backup的物理服务器上，VMware FT需要通过网络支持Test-and-Set服务。Test-and-Set请求会设置<strong>标志位</strong>，并且返回旧的值。Primary和Backup都需要获取Test-and-Set标志位，当第一个请求送达时，Test-and-Set服务会说，这个标志位之前是0，现在是1。第二个请求送达时，Test-and-Set服务会说，标志位已经是1了，你不允许成为Primary。</p>
]]></content>
      <tags>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title>test</title>
    <url>/2022/07/23/test/</url>
    <content><![CDATA[]]></content>
      <tags>
        <tag>日常</tag>
      </tags>
  </entry>
  <entry>
    <title>故障模型</title>
    <url>/2023/02/12/%E6%95%85%E9%9A%9C%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[<h1 id="故障模型"><a href="#故障模型" class="headerlink" title="故障模型"></a>故障模型</h1><p>最近在学习论文VM-FT，涉及到故障类型fail-stop failure，故在此对故障类型进行总结</p>
<h2 id="fault-error-and-failure"><a href="#fault-error-and-failure" class="headerlink" title="fault, error and failure"></a>fault, error and failure</h2><p>·Fault: 在系统中某一个步骤偏离正确的执行叫做一个fault, 比如内存写入错误, 但是如果内存是ECC的那么这个fault可以立刻被修复, 就不会导致error。</p>
<p>·Error: 如果一个fault没能在结果影响到整个系统状态之前被修复, 结果导致系统的状态错误, 那么这就是一个error, 比如不带ECC的内存导致一个计算结果错误。</p>
<p>·Failure: 如果一个系统的error没能在错误状态传递给其它节点之前被修复, 换句话说error被扩散出去, 这就是一个failure.</p>
<p>所以他们的关系是fault导致error, error导致failure. 在分布式系统中, 每个节点很难确定其它节点内部的状态, 通常只能通过和其他节点的交互监测到failure. 接下来我们所说的故障一般都是指failure.</p>
<h2 id="分布式系统故障模型"><a href="#分布式系统故障模型" class="headerlink" title="分布式系统故障模型"></a>分布式系统故障模型</h2><p>Byzantine failures: 这是最难处理的情况, 一个节点压根就不按照程序逻辑执行, 对它的调用会返回给你随意或者混乱的结果. 要解决拜占庭式故障需要有同步网络, 并且故障节点必须小于1&#x2F;3或者消息传递过程中不可篡改，通常只有某些特定领域才会考虑这种情况通过高冗余来消除故障。</p>
<p>Crash-recovery failures: 它比byzantine类故障加了一个限制, 那就是节点总是按照程序逻辑执行, 结果是正确的. 但是不保证消息返回的时间. 原因可能是crash后重启了, 网络中断了, 异步网络中的高延迟. 对于crash的情况还要分健忘(amnesia)和非健忘的两种情况. 对于健忘的情况, 是指这个crash的节点重启后没有完整的保存crash之前的状态信息, 非健忘是指这个节点crash之前能把状态完整的保存在持久存储上, 启动之后可以再次按照以前的状态继续执行和通信。</p>
<p>Omission failures: 比crash-recovery多了一个限制, 就是一定要非健忘. 有些算法要求必须是非健忘的. 比如最基本版本的Paxos要求节点必须把ballot number记录到持久存储中, 一旦crash, 修复之后必须继续记住之前的ballot number。</p>
<p>Crash failure: Omission failure的特例；在omission failure的基础上，增加了节点停止响应的假设，也即持续性地omission failure。</p>
<p>Crash-stop failures&#x2F;Fail-stop failures: 也叫做crash failure, 它比omission failure多了一个故障发生后要停止响应的要求. 比如一个节点出现故障后立即停止接受和发送所有消息, 或者网络发生了故障无法进行任何通信, 并且这些故障不会恢复. 简单讲, 一旦发生故障, 这个节点 就不会再和其它节点有任何交互. 就像他的名字描述的那样, crash and stop。</p>
<p><img data-src="https://github.com/coder-Tinghao/coder-Tinghao.github.io/blob/main/images/Failure_modes.png?raw=true" alt="故障模型"></p>
<p>Fail-stop failure这个模型，其是在Crash failure模型的基础上增加了故障可检测的假设。回顾我们在第2节中的理解：同步网络和异步网络最大的区别在于故障的可检测性。<br>因此，Fail-stop failure模型本质上是在Crash failure模型的基础上增加了“同步网络”的假设</p>
<h2 id="fail-stop-故障-和-bugs"><a href="#fail-stop-故障-和-bugs" class="headerlink" title="fail-stop 故障 和 bugs"></a>fail-stop 故障 和 bugs</h2><p>简单来说复制能够处理单台计算机的fail-stop故障，如果某些东西出现故障，只是单纯的停止运行，而不是运算出错误结果。 比如电源线、服务器风扇导致CPU过热停止运行、网络等故障。网络隔离场景很有趣，因为从外界来看，这和服务器停止运行毫无区别。</p>
<p>复制不能处理软件中的bug和硬件设计中的缺陷。 如果相同的程序，都计算出错误的结果，那么再多副本也都无法避免。我们也不能期望复制可以处理硬件的漏洞，当硬件有漏洞的时候会计算出错误的结果，至少基于复制这种技术，我们就无能为力了。</p>
]]></content>
      <tags>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title>拜占庭容错--BFT</title>
    <url>/2022/12/25/%E6%8B%9C%E5%8D%A0%E5%BA%AD%E5%AE%B9%E9%94%99-BFT/</url>
    <content><![CDATA[<h1 id="拜占庭容错系统中的N-gt-x3D-3f-1问题"><a href="#拜占庭容错系统中的N-gt-x3D-3f-1问题" class="headerlink" title="拜占庭容错系统中的N &gt;&#x3D; 3f+1问题"></a>拜占庭容错系统中的<strong>N &gt;&#x3D; 3f+1</strong>问题</h1><p><a href="https://decentralizedthoughts.github.io/start-here/">https://decentralizedthoughts.github.io/start-here/</a></p>
<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>最近在上面的网站中学习区块链基础知识，昨天从<a href="https://zhuanlan.zhihu.com/p/36000412">https://zhuanlan.zhihu.com/p/36000412</a>看懂了N &gt;&#x3D; 3f+1的证明，在此进行总结：<br><br>解决拜占庭将军问题的共识算法有很多（如PBFT，QU，HQ等），但无论共识算法如何设计，所有的算法都需要遵守一个容忍攻击者数量的限制(这个限制是与某个具体的算法实现无关的)。该限制描述如下：结点总数量为N时，最多只能容纳f个拜占庭故障（叛徒）节点，令N &gt;&#x3D; 3f+1。</p>
<h2 id="证明"><a href="#证明" class="headerlink" title="证明"></a>证明</h2><h3 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h3><p>quorum（法定人数），所谓quorum指的是做出一次决策至少需要的同意票数量<br><br>liveness（活性）：liveness这个术语来自论文<a href="https://link.zhihu.com/?target=https://groups.csail.mit.edu/tds/papers/Lynch/jacm85.pdf">FLP impossibility</a>。liveness，又称guaranteed termination，就是说共识算法的执行过程中不能卡死，最终能按照算法流程一步步得到执行结果。<br><br>safety（安全性）:safety这个术语同样来自论文FLP impossibility。此处用通俗的话解释一下，所谓安全性又称linearizability，就是说，执行共识算法之后，所有节点的内容能保证一致</p>
<h3 id="推导"><a href="#推导" class="headerlink" title="推导"></a>推导</h3><p>设总结点数为N，作恶的拜占庭节点数为 f，法定人数为Q<br><br>要满足liveness必须有：<br><br>**Q &lt;&#x3D; N-f** <br><br>说明：如果共识算法需要的Q大于N-f，则当f个拜占庭故障节点都主动破坏时，算法必然不能执行下去<br></p>
<p>要满足safety必须有：<br><br><strong>2Q - N &gt; f</strong> <br><br>说明：假设好人的意见分裂了，X个人倾向于方案A，剩余N-f-X个人倾向于方案B。根据safety的定义，坏人要使系统不safety，就要利用自己的f张投票使得方案A和B都能通过：X+f≥Q；N-f-X+f≥Q 。两式相加得证N+f≥2Q 是坏人能作恶的边界条件。</p>
<p>因为除了leader节点之外的好人节点有可能会出现运行异常，对外表现为意见分裂。但好人节点与拜占庭节点最大的不同是，拜占庭节点可能同时支持正反两种观点，而好人节点只可能分裂（有人支持正有人支持反），但不会说两样话（即，既支持正也支持反）</p>
<p>因此：</p>
<p>N+f &lt; 2Q &lt;&#x3D; 2(N-f)<br><br>N &gt; 3f<br><br>if N&#x3D;3f+1 此时Q的最小值为：<br><br>Qmin &#x3D; 2f+1</p>
<p>证毕</p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>对于所有的拜占庭将军问题，在不更改假设前提时，无论算法怎么设计，当总结点数为N时，最多只能容忍f个作恶节点(拜占庭故障节点)，使N &gt; 3f + 1，此时Q &gt; 2f + 1</p>
]]></content>
      <tags>
        <tag>共识算法</tag>
      </tags>
  </entry>
  <entry>
    <title>纠删码（Erasure Code）及其演进LRC（Locally Repairable Codes）</title>
    <url>/2023/01/11/%E7%BA%A0%E5%88%A0%E7%A0%81%EF%BC%88Erasure-Code%EF%BC%89%E5%8F%8A%E5%85%B6%E6%BC%94%E8%BF%9BLRC%EF%BC%88Locally-Repairable-Codes%EF%BC%89/</url>
    <content><![CDATA[<h1 id="EC及其演进LRC"><a href="#EC及其演进LRC" class="headerlink" title="EC及其演进LRC"></a>EC及其演进LRC</h1><p>近期因为几门课程作业，阅读了一些论文，在此依次进行总结</p>
<p><a href="https://zhuanlan.zhihu.com/p/69374970">参考链接1</a></p>
<p><a href="https://www.cnblogs.com/tinoryj/p/Erasure-Codes-for-Storage-Systems-Summary.html">参考链接2</a></p>
<p><a href="https://blog.csdn.net/cyq6239075/article/details/105775698">参考链接3</a></p>
<p>如何保证存储可靠性、数据可用性是大规模存储系统的难点和要点。数据冗余是保障存储可靠性、数据可用性的最有效手段。传统的冗余机制主要有副本（Replication）和纠删编码（Erasure Code）</p>
<h2 id="副本（Replication）"><a href="#副本（Replication）" class="headerlink" title="副本（Replication）"></a>副本（Replication）</h2><p>副本是将每个原始数据分块都镜像复制到另一存储介质上，从而保证在原始数据失效后，数据仍然可用并能通过副本数据恢复。多副本策略即将数据存储多个副本（一般是三副本，比如HDFS），当某个副本丢失时，可以通过其他副本复制回来。三副本的磁盘利用率为1&#x2F;3。</p>
<h2 id="纠删编码（EC）"><a href="#纠删编码（EC）" class="headerlink" title="纠删编码（EC）"></a>纠删编码（EC）</h2><h3 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h3><p>总数据块 &#x3D; 原始数据块 + 校验块</p>
<p>n &#x3D; k + m</p>
<p>从k个原始数据块中计算出m个校验块</p>
<p>编码只需要乘法和加法，解码需要用高斯消除或矩阵求逆求的方法解一组线性方程。</p>
<h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><p>在出现硬盘故障后，重建数据非常耗CPU，而且计算一个数据块需要通过网络读出k倍的数据并传输，网络负载成倍增加</p>
<h2 id="局部校验编码LRC（Locally-Repairable-Codes）"><a href="#局部校验编码LRC（Locally-Repairable-Codes）" class="headerlink" title="局部校验编码LRC（Locally Repairable Codes）"></a>局部校验编码LRC（Locally Repairable Codes）</h2><p>局部校验编码：将校验块（parity block）分为全局校验块(global parity)、局部重建校验块(local reconstruction parity)，故障恢复时分组计算。</p>
<p>例子：</p>
<p>P1&#x3D;D1+D2</p>
<p>P2&#x3D;D3+D4</p>
<p>P3&#x3D; D1+D2+ D3+D4</p>
<p>P1、P2是局部校验。P3是全局校验<br><img data-src="https://img-blog.csdnimg.cn/20200426195932950.png" alt="例子"></p>
<p>当仅损坏一个数据块时，可以根据该数据在所在的分组，在组内对该数据块进行重建。损坏一个数据块时的最差情况就是全局校验块损坏，此时需要读全部数据块数据进行重建。</p>
<p>当损坏两个数据块时情况分成组内和组外，不管哪种需要拉取全部数据进行重建，此时采用LRC方法并不能提升性能。</p>
<p>当损坏三块数据时，如果这三个数据块在一个组内，例如P1、D1、D2。此时就无法进行数据重建，虽然我们这里是使用三个校验。</p>
<p>所以LRC并不是100%保证数据不丢，并且还要多占用一部分存储空间</p>
]]></content>
      <tags>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title>消息认证码Message Authentication Code(MAC)</title>
    <url>/2023/03/19/%E6%B6%88%E6%81%AF%E8%AE%A4%E8%AF%81%E7%A0%81Message-Authentication-Code-MAC/</url>
    <content><![CDATA[<p>消息认证码（Message Authentication Code）是一种确认完整性并进行认证的技术</p>
<p>消息认证码的输入为任意长度的<strong>消息</strong>和一个发送者与接收者之间<strong>共享的密钥</strong>，它可以输出固定长度的数据，这个数据称为 MAC 值</p>
<p><img data-src="https://github.com/coder-Tinghao/coder-Tinghao.github.io/blob/main/images/message_authentication_code.png?raw=true" alt="消息认证码"></p>
<p>个人理解，消息认证码就是：传递一条消息，为了保证这个消息是你本人发的，把这个消息和共享密钥结合得到一个MAC值，同时传递过去，接收方通过接收消息，计算MAC值再与发送<br>方发来的MAC值对比即可验证。</p>
<p><img data-src="https://github.com/coder-Tinghao/coder-Tinghao.github.io/blob/main/images/message_authentication_code_example.png?raw=true" alt="例子"></p>
]]></content>
      <tags>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title>记hexo配置与部署</title>
    <url>/2022/07/26/%E8%AE%B0hexo%E9%85%8D%E7%BD%AE%E4%B8%8E%E9%83%A8%E7%BD%B2/</url>
    <content><![CDATA[<h1 id="记hexo配置与部署"><a href="#记hexo配置与部署" class="headerlink" title="记hexo配置与部署"></a>记hexo配置与部署</h1><p>hexo中文文档：<a href="https://hexo.io/zh-cn/docs/">https://hexo.io/zh-cn/docs/</a></p>
<hr>
<p>因为官方文档的一点小问题和我对Mac系统环境变量、终端类型等的不熟悉，这个安装过程比较坎坷。关于Git和Node.js的安装比较顺利，我又顺手安装了Homebrew（Mac下的包管理器）。</p>
<hr>
<h2 id="Hexo安装过程中的小问题"><a href="#Hexo安装过程中的小问题" class="headerlink" title="Hexo安装过程中的小问题"></a>Hexo安装过程中的小问题</h2><p>Hexo的安装首先如文档所说出现了EACCES权限错误，解决办法是将npm的<strong>全局</strong>安装包放到新创建的npm-global文件夹，并将npm-global的路径添加到环境变量，问题就出在环境变量的配置，Node.js的文档中提到将npm-global&#x2F;bin添加到环境变量的语句加入到<del>&#x2F;.profile 中，</del>&#x2F;.profile是<strong>bash终端</strong>的环境变量配置文件，而现在Mac系统默认为<strong>zsh终端</strong>，需要将上述语句加入.zshrc文件中，保证在打开终端后npm全局环境变量已经加载。否则，需要每次source ~&#x2F;.profile才可以执行 hexo &lt; command &gt; 命令。经过几个小时的排查最终发现。</p>
<hr>
<h2 id="Hexo常用命令"><a href="#Hexo常用命令" class="headerlink" title="Hexo常用命令"></a>Hexo常用命令</h2><p>hexo n “新博客”  创建新博客</p>
<p>hexo clean      清理hexo缓存</p>
<p>hexo g          生成博客</p>
<p>hexo d          部署博客</p>
]]></content>
      <tags>
        <tag>日常</tag>
      </tags>
  </entry>
  <entry>
    <title>通信模型--同步、异步、部分同步</title>
    <url>/2022/12/25/%E9%80%9A%E4%BF%A1%E6%A8%A1%E5%9E%8B-%E5%90%8C%E6%AD%A5%E3%80%81%E5%BC%82%E6%AD%A5%E3%80%81%E9%83%A8%E5%88%86%E5%90%8C%E6%AD%A5/</url>
    <content><![CDATA[<h1 id="通信模型"><a href="#通信模型" class="headerlink" title="通信模型"></a>通信模型</h1><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>在标准的分布式计算模型中，通信的不确定性被可以控制消息延迟的敌手捕获。通信模型定义了敌手延迟消息的能力的限制。</p>
<p>在同步模型中，存在一些已知的有限时间界限Δ。 对于发送的任何消息，敌手最多可以将其传递延迟Δ。</p>
<p>在异步模型中，对于发送的任何消息，敌手可以将其传递延迟任意有限的时间。 因此，一方面，传递消息的时间没有限制，但另一方面，每条消息最终都必须传递。</p>
<p>部分同步模型（参见<a href="https://groups.csail.mit.edu/tds/papers/Lynch/jacm88.pdf">DLS88</a>）旨在找到这两个模型之间的折中。 假设存在一些已知的有限时间界限 Δ 和一个称为 GST（Global Stabilization Time–全局稳定时间）的特殊事件，使得：敌手必须导致 GST 事件在某个未知的有限时间后最终发生。在时间 x 发送的任何消息必须在时间 Δ+max(x,GST) 之前交付。</p>
<p>在部分同步模型中，系统在GST之前异步运行，在GST之后同步运行</p>
<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>对于同步模型：</p>
<p>1.存在一个trade-off问题：有限时间界限Δ过长，造成长时间超时，会降低性能；有限时间界限Δ过短，会造成安全性违规问题。<br>2.假设找到了Δ的sweet spot，想象一个发送者向两个接收者广播一条消息，一个消息在Δ−ε时间后到达，另一个在Δ+ε后到达。在这里，真实世界的行为与模型不同，这可能再次导致安全问题。</p>
<p>对于异步模型：</p>
<p>异步模型的主要问题是该模型中的协议往往更复杂且更难推理。</p>
]]></content>
      <tags>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title>零知识证明ZKP</title>
    <url>/2022/12/30/%E9%9B%B6%E7%9F%A5%E8%AF%86%E8%AF%81%E6%98%8EZKP/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>事务管理与并发控制</title>
    <url>/2022/11/14/%E4%BA%8B%E5%8A%A1%E7%AE%A1%E7%90%86%E4%B8%8E%E5%B9%B6%E5%8F%91%E6%8E%A7%E5%88%B6/</url>
    <content><![CDATA[<p>最近看了看《数据库事务处理的艺术》，主要介绍事物原理和并发控制技术，之前有看过一些并发控制的博客，看的一头雾水，这本书相对更清晰一些</p>
<h1 id="事务"><a href="#事务" class="headerlink" title="事务"></a>事务</h1><h2 id="ACID特性"><a href="#ACID特性" class="headerlink" title="ACID特性"></a>ACID特性</h2><p>（A）原子性：要么成功–Committed，要么失败–Aborted</p>
<p>（C）一致性：<strong>from one valid state to another</strong>。数据在事务的操作下，一直符合“all defined rules”。一个是属于用户的语义所限定的数据一致性，一个是系统级，要求数据库系统符合可串行性（serializability）和可恢复性(recoverability),这两个将在下面具体解释</p>
<p>（I）隔离性：存在多个事务（多个会话中的不同的但同一时间段内运行的事务）同时运行，但他们运行的顺序好像是”serially“</p>
<p>（D）持久性：committed的数据，要能够永久保存</p>
<h2 id="事务的属性"><a href="#事务的属性" class="headerlink" title="事务的属性"></a>事务的属性</h2><h3 id="可串行化"><a href="#可串行化" class="headerlink" title="可串行化"></a>可串行化</h3><p><strong>保证并发的事务调度方式既能满足数据一致性需求，又能提高并发事务的执行效率</strong></p>
<p>如果事务间没有共同的操作对象（R&#x2F;W操作），则事务之间的执行顺序前后置换是没有关系的；但是如果事务间存在共同的操作对象，则事务间先后执行的顺序需要区分；对于存在共同操作对象的多个并发执行的事务，如果其执行等价于某个串行化调度，则这个调度是可串行化调度，具有了可串性化属性。可串性化保证的是多个事务并发时执行顺序要对数据的一致性没有影响。</p>
<p>关于等价，需要几个概念：冲突行为、冲突等价、冲突可串行化<br>冲突可串行化：某个调度“冲突等价”于一个或多个串行调度</p>
<h3 id="可恢复性"><a href="#可恢复性" class="headerlink" title="可恢复性"></a>可恢复性</h3><p>已经提交的事务没有读过被中止的事务的写数据，可恢复性保证多个事务并发调度后期的提交顺序对数据的一致性没有影响</p>
<h3 id="严格性"><a href="#严格性" class="headerlink" title="严格性"></a>严格性</h3><p>保证有冲突动作的并发事务中，先发生写操作的事务提交或中止的操作优先于其他事务。</p>
<h1 id="隔离级别"><a href="#隔离级别" class="headerlink" title="隔离级别"></a>隔离级别</h1><p>Serializable（串行化）<br>以物理上是可串行化的机制保证逻辑上符合串行化调度。一个事务在执行过程中完全看不到其他事务的对数据库所做的更新</p>
<p>Repeatable Read（可重复读）<br>一个事务在执行过程中可以看到其他事务已经提交的新插入的元组，但是不能看到其他事务对已有元组的更新。对于<strong>读出的记录</strong>，添加共享锁直到事务T1结束。其他事务T2对这个记录的修改会一直等待直到事务T1结束。但其他事务允许读取同样的数据。</p>
<p>Read Committed（已提交读）<br>一个事务在执行过程中可以看到<strong>已经提交</strong>的其他事务新插入的元组，能看到<strong>已经提交</strong>的其他事务对已有元组的更新</p>
<p>Read Uncommitted（未提交读）<br>一个事务在执行过程中可以看到<strong>没有提交</strong>的其他事务新插入的元组，能看到<strong>没有提交</strong>的其他事务对已有元组的更新</p>
<h1 id="快照隔离"><a href="#快照隔离" class="headerlink" title="快照隔离"></a>快照隔离</h1><p>使用快照隔离技术的事务中的所有读操作，读到的数据一定是一致的<br>避免了各种<strong>读异常现象</strong><br>如果没有写-写冲突，则会提交成功。不会发生读-写、写-读冲突<br>从事务开始时，处于<strong>当时的并发事务的状态（快照）</strong>被保存，利用这个快照可以判断本事务和其他事务之间启动的先后顺序，事务的读写情况。</p>
<p>快照隔离是MVCC技术的一种实现方式，MVCC技术的本质，是为每个对象在写操作发生时，生成一个新的版本；在读操作发生时，读出最近的一个版本。<br>并发事务同时写一个数据项，要遵循“First-Committer-Wins”,并发的同时写同一个数据项的事务只能有一个成功，另外一个必须回滚，相当于<strong>并发不存在</strong>，解决了写-写冲突。<br>存在写偏序问题，不能保证数据的一致性。<br>阅读进阶GSI–《Database Replication Using Generalized Snapshot Isolation》、PCSI、《Generalized Snapshot Isolation and a Prefix-Consistent Implementation》</p>
<h1 id="并发控制"><a href="#并发控制" class="headerlink" title="并发控制"></a>并发控制</h1><h2 id="并发控制的实现策略"><a href="#并发控制的实现策略" class="headerlink" title="并发控制的实现策略"></a>并发控制的实现策略</h2><h3 id="乐观（OCC）"><a href="#乐观（OCC）" class="headerlink" title="乐观（OCC）"></a>乐观（OCC）</h3><h3 id="悲观（PCC）"><a href="#悲观（PCC）" class="headerlink" title="悲观（PCC）"></a>悲观（PCC）</h3><h2 id="并发控制的实现技术"><a href="#并发控制的实现技术" class="headerlink" title="并发控制的实现技术"></a>并发控制的实现技术</h2><h3 id="时间戳（TO）"><a href="#时间戳（TO）" class="headerlink" title="时间戳（TO）"></a>时间戳（TO）</h3><ul>
<li>写-读冲突 事务早于写操作，读要回滚；事务晚于写操作，读不受影响</li>
<li>读-写冲突 读前写后回滚写</li>
<li>写-写冲突 新写回滚</li>
<li>改进：Thomas写法则</li>
</ul>
<h3 id="基于有效性检查的并发控制方法"><a href="#基于有效性检查的并发控制方法" class="headerlink" title="基于有效性检查的并发控制方法"></a>基于有效性检查的并发控制方法</h3><p>分为：读阶段、有效性检查阶段、写阶段<br>读阶段先对局部变量进行修改（本地缓存），有效性检查，将局部变量复制到数据库中</p>
<h3 id="Commitment-ordering-CO"><a href="#Commitment-ordering-CO" class="headerlink" title="Commitment ordering(CO)"></a>Commitment ordering(CO)</h3><h3 id="串行化图形检测"><a href="#串行化图形检测" class="headerlink" title="串行化图形检测"></a>串行化图形检测</h3><h3 id="两阶段封锁（2PL）"><a href="#两阶段封锁（2PL）" class="headerlink" title="两阶段封锁（2PL）"></a>两阶段封锁（2PL）</h3><ul>
<li>2PL</li>
<li>S2PL<br>事务持有的排他锁必须在事务提交后才能释放</li>
<li>SS2PL<br>事务提交之前不得释放任何锁</li>
</ul>
<h3 id="多版本并发控制技术（MVCC）"><a href="#多版本并发控制技术（MVCC）" class="headerlink" title="多版本并发控制技术（MVCC）"></a>多版本并发控制技术（MVCC）</h3><ul>
<li>多版本时间戳排序</li>
<li>多版本两阶段封锁协议</li>
<li>基于MVCC的可串行化快照隔离并发控制方法（SSI）<br>SSI&#x3D; SS2PL+MVCC(+SI)+SIREAD锁<br>根据三种依赖关系：读写、写读、写写画并发事务依赖图</li>
</ul>
<h3 id="基于索引的并发控制技术"><a href="#基于索引的并发控制技术" class="headerlink" title="基于索引的并发控制技术"></a>基于索引的并发控制技术</h3><ul>
<li>谓词锁</li>
</ul>
<h1 id="日志技术与恢复子系统"><a href="#日志技术与恢复子系统" class="headerlink" title="日志技术与恢复子系统"></a>日志技术与恢复子系统</h1>]]></content>
      <tags>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title>Paxos原理</title>
    <url>/2023/04/04/Paxos%E5%8E%9F%E7%90%86/</url>
    <content><![CDATA[<h1 id="Paxos共识协议"><a href="#Paxos共识协议" class="headerlink" title="Paxos共识协议"></a>Paxos共识协议</h1><p>Paxos就是一个在<strong>异步通信</strong>环境，并容忍在只有<strong>多数派</strong>机器存活的情况下，仍然能完成一个一致性写入的协议</p>
<p>学习过程比较痛苦，multi-Paxos根本看不懂…以后有机会再补充</p>
<h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><p><a href="http://dinghao.li/2018/12/Paxos/">共识算法系列：Paxos&#x2F;Multi-Paxos算法关键点综述、优缺点总结</a><br><a href="https://mp.weixin.qq.com/s?__biz=MzI4NDMyNTU2Mw==&mid=2247483695&idx=1&sn=91ea422913fc62579e020e941d1d059e#rd">微信自研生产级paxos类库PhxPaxos实现原理介绍</a><br><a href="https://zhuanlan.zhihu.com/p/21438357">Paxos理论介绍(1): 朴素Paxos算法理论推导与证明</a></p>
<h2 id="Paxos算法基本概念"><a href="#Paxos算法基本概念" class="headerlink" title="Paxos算法基本概念"></a>Paxos算法基本概念</h2><p>Paxos的目标——<strong>“确定一个值”、“确定多个值”、“有序的确定多个值”</strong><br>Paxos协议中的角色包括：<strong>Acceptor、Proposer、Learner、State machine</strong><br>Proposer——提出提议；Acceptor——接受提议；Learner——做instance对齐</p>
<p>Proposer只需要与<strong>多数派</strong>的Acceptor交互，即可完成一个值的确定，但一旦这个值被确定下来后，无论Proposer再发起任何值的写入，Data数据都不会再被修改。Chosen value即是被确定的值，永远不会被修改。 </p>
<p><strong>有序的确定多个值</strong>：<br>只要我们通过paxos完成一个<strong>多机一致的有序的操作系列</strong>，就能保证Paxos算法的一致性：</p>
<p>1 给实例一个编号，定义为i，i从0开始，只增不减，由本机器生成，不依赖网络<br>2 我们保证一台机器任一时刻只能有一个实例在工作<br>3 当编号为i的实例获知已经确定好一个值之后，这个实例将会被销毁，进而产生一个编号为i+1的实例</p>
<p><strong>实例的对齐</strong>：<br>回去询问别的机器的相同编号的实例<br>如果这个实例已经被销毁了，那说明值已经确定好了，直接把这个值拉回来写到当前实例里面，直接由Learner直接学习得到即可。</p>
<p><strong>状态机</strong><br>状态机必须记录下来输入过的最大实例编号<br><strong>启动重放</strong>：把这些chosen value一个一个输入到状态机，那么状态机的状态就会更新到y了</p>
<h2 id="朴素Paxos算法"><a href="#朴素Paxos算法" class="headerlink" title="朴素Paxos算法"></a>朴素Paxos算法</h2><p>个人理解朴素Paxos算法就是做一个instance中值的确定的算法过程。</p>
<p>其中包括三个投票的约束，通过反证法可以证明得到一致性结论，比较重要的是MaxVote的定义</p>
<p>问题：为什么需要多轮投票？</p>
<h2 id="Multi-Paxos算法"><a href="#Multi-Paxos算法" class="headerlink" title="Multi-Paxos算法"></a>Multi-Paxos算法</h2><p>暂时没看懂</p>
]]></content>
      <tags>
        <tag>共识算法</tag>
      </tags>
  </entry>
  <entry>
    <title>Exponential backoff--指数补偿</title>
    <url>/2023/05/02/Exponential-backoff-%E6%8C%87%E6%95%B0%E8%A1%A5%E5%81%BF/</url>
    <content><![CDATA[<h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><p><a href="https://en.wikipedia.org/wiki/Exponential_backoff">wiki百科</a><br><a href="http://note.huangz.me/algorithm/arithmetic/exponential-backoff.html">指数补偿 —— Exponential backoff</a></p>
<h2 id="指数补偿"><a href="#指数补偿" class="headerlink" title="指数补偿"></a>指数补偿</h2><p>指数补偿指的是，在执行事件时，通过反馈，逐渐降低某个过程的速率，从而最终<strong>找到一个合适的速率</strong>（来处理事件）。</p>
<p>指数补偿通常用于网络和传输协议，比如在进行网络连接时，如果第一次请求失败，那么可以等待 t1之后重试，如果再次请求还是失败，那么等待 t2之后重试。</p>
<p>重试可以一直继续下去，或者等待次数或等待时间超过特定值为止。</p>
<p>等待的时间 tn可以是随机选择，也可以随着重试的次数而逐渐加大，诸如此类。</p>
<p>指数退避算法是闭环控制系统的一种形式，可降低受控过程响应不良事件的速率。例如，如果智能手机应用程序无法连接到其服务器，它可能会在 1 秒后重试，如果再次失败，则在 2 秒后，然后 4 等。每次暂停都乘以固定数量（在此情况 2)。在这种情况下，不利事件是无法连接到服务器。不良事件的其他示例包括网络流量冲突、来自服务的错误响应或降低速率的明确请求（即“回退”）。</p>
<h2 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h2><p>《UNIX 环境高级编程，第二版》（APUE，2E） 16.4 节提供了一个带重试的 socket 连接程序， 如果连接失败， 那么程序就睡眠一段时间再尝试， 每失败一次睡眠的时间就延长一些：</p>
<pre><code>#include &quot;apue.h&quot;
#include &lt;sys/socket.h&gt;

#define MAXSLEEP 128

int connect_retry(int sockfd, const struct sockaddr *addr, socklen_talen)
&#123;
int nsec;

for (nsec = 1; nsec &lt; MAXSLEEP; nsec &lt;&lt;=1) &#123;
    if (connect(sockfd, addr, alen) == 0) &#123;
        // connect accepted.
        return 0;
    &#125;
    if (nsec &lt;= MAXSLEEP/2)
        sleep(nsec);
    &#125;

    return -1;
&#125;
</code></pre>
]]></content>
      <tags>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title>linux-futex</title>
    <url>/2023/05/18/linux-futex/</url>
    <content><![CDATA[<h1 id="Futex"><a href="#Futex" class="headerlink" title="Futex"></a>Futex</h1><p> futex：a sort of fast, user-space mutual exclusion primitive.</p>
<h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><p><a href="https://www.jianshu.com/p/d534f6c1fc5d">futex机制介绍</a><br><a href="https://zhuanlan.zhihu.com/p/454244967">内核实现大致逻辑</a></p>
<h2 id="机制介绍"><a href="#机制介绍" class="headerlink" title="机制介绍"></a>机制介绍</h2><p>Futex是一种用户态和内核态混合的同步机制。首先，同步的进程间通过mmap共享一段内存，futex变量就位于这段共享的内存中且操作是原子的，<strong>当进程尝试进入互斥区或者退出互斥区的时候，先去查看共享内存中的futex变量，如果没有竞争发生，则只修改futex,而不用再执行系统调用了</strong>。当通过访问futex变量告诉进程有竞争发生，则还是得执行系统调用去完成相应的处理(wait 或者 wake up)。简单的说，futex就是通过在用户态的检查，（motivation）如果了解到没有竞争就不用陷入内核了，大大提高了low-contention时候的效率</p>
<h3 id="为什么要有futex，他解决什么问题？何时加入内核的？"><a href="#为什么要有futex，他解决什么问题？何时加入内核的？" class="headerlink" title="为什么要有futex，他解决什么问题？何时加入内核的？"></a>为什么要有futex，他解决什么问题？何时加入内核的？</h3><p>经研究发现，很多同步是无竞争的，即某个进程进入互斥区，到再从某个互斥区出来这段时间，常常是没有进程也要进这个互斥区或者请求同一同步变量的。但是在这种情况下，这个进程也要陷入内核去看看有没有人和它竞争，退出的时侯还要陷入内核去看看有没有进程等待在同一同步变量上。这些不必要的系统调用(或者说内核陷入)造成了大量的性能开销。为了解决这个问题，Futex就应运而生。</p>
<h2 id="futex-wait-和futex-wake"><a href="#futex-wait-和futex-wake" class="headerlink" title="futex_wait 和futex_wake"></a>futex_wait 和futex_wake</h2><p>总地来说，futex 包含两种基本操作（futex_op）：futex_wait 和futex_wake ：</p>
<p><strong>FUTEX_WAIT</strong> 判断保存在地址addr 的值是否等于val，如果等于，则将当前线程休眠，不等于则返回错误码EWOULDBLOCK。</p>
<p><strong>FUTEX_WAKE</strong> 则是唤醒在地址addr上数量为val个线程</p>
<p>uaddr 和 val 是futex 最重要的两个变量，uaddr 是一个4 字节大小值（futex word）的地址，这个地址一般通过共享内存由多个进（线）程共享，<strong>每个进（线）程在进入futex()后将判断该地址的值和自己期望的值val （expected value）是否相同</strong></p>
<p><strong>具体实现步骤见上文链接</strong></p>
]]></content>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title>自旋锁——spinlock</title>
    <url>/2023/06/08/%E8%87%AA%E6%97%8B%E9%94%81%E2%80%94%E2%80%94spinlock/</url>
    <content><![CDATA[<p><a href="https://www.cnblogs.com/cxuanBlog/p/11679883.html">参考链接</a></p>
<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>由于在多处理器环境中某些资源的有限性，有时需要互斥访问(mutual exclusion)，这时候就需要引入锁的概念，只有获取了锁的线程才能够对资源进行访问，由于多线程的核心是CPU的时间分片，所以同一时刻只能有一个线程获取到锁。那么就面临一个问题，那么没有获取到锁的线程应该怎么办？</p>
<p>通常有两种处理方式：<br>一种是没有获取到锁的线程就一直循环等待判断该资源是否已经释放锁，这种锁叫做<strong>自旋锁</strong>，它不用将线程阻塞起来(NON-BLOCKING)；<br>还有一种处理方式就是把自己阻塞起来，等待重新调度请求，这种叫做<strong>互斥锁</strong>。</p>
<h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p><strong>自旋锁</strong>的定义：当一个线程尝试去获取某一把锁的时候，如果这个锁此时已经被别人获取(占用)，那么此线程就无法获取到这把锁，该线程将会等待，间隔一段时间后会再次尝试获取。这种采用<strong>循环加锁 -&gt; 等待的机制</strong>被称为自旋锁(spinlock)。</p>
<p><img data-src="https://github.com/coder-Tinghao/coder-Tinghao.github.io/blob/main/images/spinlock.png?raw=true" alt="自旋锁示意图"></p>
<p>如果持有锁的线程能<strong>在短时间内释放锁资源</strong>，那么那些等待竞争锁的线程就不需要做内核态和用户态之间的切换进入阻塞状态，它们只需要等一等(自旋)，等到持有锁的线程释放锁之后即可获取，这样就避免了用户进程和内核切换的消耗。</p>
<p>自旋锁尽可能的减少线程的阻塞，这对于锁的竞争不激烈，且占用锁时间非常短的代码块来说性能能大幅度的提升，因为自旋的消耗会小于线程阻塞挂起再唤醒的操作的消耗，这些操作会导致线程发生两次上下文切换！</p>
<p>但是如果锁的竞争激烈，或者持有锁的线程需要长时间占用锁执行同步块，这时候就不适合使用自旋锁了，因为自旋锁在获取锁前一直都是占用 cpu 做无用功，同时有大量线程在竞争一个锁，会导致获取锁的时间很长，线程自旋的消耗大于线程阻塞挂起操作的消耗，其它需要 cpu 的线程又不能获取到 cpu，造成 cpu 的浪费。所以这种情况下我们要关闭自旋锁。</p>
<h2 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h2><pre><code>public class SpinLockTest &#123;

    private AtomicBoolean available = new AtomicBoolean(false);

    public void lock()&#123;

        // 循环检测尝试获取锁
        while (!tryLock())&#123;
            // doSomething...
        &#125;

    &#125;

    public boolean tryLock()&#123;
        // 尝试获取锁，成功返回true，失败返回false
        return available.compareAndSet(false,true);
    &#125;

    public void unLock()&#123;
        if(!available.compareAndSet(true,false))&#123;
            throw new RuntimeException(&quot;释放锁失败&quot;);
        &#125;
    &#125;

&#125;
</code></pre>
<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p><strong>无法保证多线程竞争的公平性</strong></p>
]]></content>
      <tags>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title>经典原子操作——atomic operation</title>
    <url>/2023/06/10/%E7%BB%8F%E5%85%B8%E5%8E%9F%E5%AD%90%E6%93%8D%E4%BD%9C%E2%80%94%E2%80%94atomic-operation/</url>
    <content><![CDATA[]]></content>
  </entry>
</search>
